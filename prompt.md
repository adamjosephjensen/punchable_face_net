notes:
- having an undo button would be nice
- it'd be nice to have a `flag` button where I can mark images that contain problems with the dataset. some of the men are labelled as women. <musing>In general: I wonder how many public data sets contain errors like this and what acceptable error rates are.</musing>
- I am noticing that this subjective task is difficult. How punchable should they be in order to be marked as punchable? how to hold a consistent standard? is it possible to prompt a multi-modal model for what it thinks of as punchable?
- in general I think I may actually have to scrape images of ceos and label them all by hand. hope not but that's what it feels like.
- it seems like a lot of these images have weird artifacts where the pixels are "smeared" up or down 
- I think it is important to define in advance how we will interpret the results of the experiment. what does success look like? what will we see if whatever model we make is not learning a generalizable function?
